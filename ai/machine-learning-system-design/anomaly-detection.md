# Anomaly Detection



Sources:

* https://docs.datadoghq.com/monitors/types/anomaly/
* https://www.datadoghq.com/blog/watchdog/

### å¼€åœºæ€»ç»“

**English:**\
For anomaly detection on a latency time series, I would design the solution as a pipeline with four key stages: **pre-processing, detection model, scoring, and post-processing**. This structure ensures the system is both accurate and practical in production.

**ä¸­æ–‡:**\
é’ˆå¯¹å»¶è¿Ÿç±»æ—¶é—´åºåˆ—çš„å¼‚å¸¸æ£€æµ‹ï¼Œæˆ‘ä¼šè®¾è®¡ä¸€ä¸ªåŒ…å«å››ä¸ªä¸»è¦é˜¶æ®µçš„æµæ°´çº¿ï¼š**é¢„å¤„ç†ã€æ£€æµ‹æ¨¡å‹ã€æ‰“åˆ†æœºåˆ¶å’Œåå¤„ç†**ã€‚è¿™ç§ç»“æ„æ—¢ä¿è¯äº†æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œä¹Ÿæ–¹ä¾¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è½åœ°ã€‚

{% stepper %}
{% step %}
### 1. Pre-processingï¼ˆé¢„å¤„ç†ï¼‰

**English:**\
Latency signals usually contain missing values, noise, and seasonal patterns like daily cycles. So the first step is to clean and normalize the data. I would fill missing points with interpolation, smooth sudden spikes if theyâ€™re artifacts, and decompose the series to remove seasonality, for example using STL decomposition. This ensures the model focuses on real anomalies instead of predictable fluctuations.

**ä¸­æ–‡:**\
å»¶è¿Ÿä¿¡å·é€šå¸¸åŒ…å«ç¼ºå¤±å€¼ã€å™ªå£°ï¼Œä»¥åŠåƒæ—¥å‘¨æœŸè¿™æ ·çš„å­£èŠ‚æ€§æ¨¡å¼ã€‚æ‰€ä»¥ç¬¬ä¸€æ­¥æ˜¯æ¸…ç†å’Œæ ‡å‡†åŒ–æ•°æ®ã€‚æˆ‘ä¼šç”¨æ’å€¼æ³•å¡«è¡¥ç¼ºå¤±ç‚¹ï¼Œå¯¹å¶ç„¶çš„å™ªå£°å°–å³°è¿›è¡Œå¹³æ»‘ï¼Œå¹¶ç”¨ STL åˆ†è§£ç­‰æ–¹æ³•å»é™¤å­£èŠ‚æ€§ã€‚è¿™æ ·æ¨¡å‹å°±èƒ½èšç„¦åœ¨çœŸæ­£çš„å¼‚å¸¸ï¼Œè€Œä¸æ˜¯å¯é¢„æµ‹çš„æ³¢åŠ¨ã€‚

* åŠ¨æœºï¼šåŸå§‹ latency æ•°æ®å¯èƒ½åŒ…å«å™ªå£°ã€ç¼ºå¤±å€¼ã€å¼‚å¸¸ spikesã€‚
* åšæ³•ï¼š
  * å»é™¤ outliers æˆ–å¹³æ»‘ (moving average, EWMA)ã€‚
  * ç¼ºå¤±å€¼å¡«è¡¥ï¼ˆlinear interpolation, forward fillï¼‰ã€‚
  * å½’ä¸€åŒ–/æ ‡å‡†åŒ– (z-score, min-max)ã€‚
  * æå–å‘¨æœŸæ€§ç‰¹å¾ (hour-of-day, day-of-week)ã€‚
* é¢è¯•ç¤ºä¾‹ï¼šæ¯”å¦‚ latency é€šå¸¸æœ‰æ—¥å‘¨æœŸï¼Œæˆ‘ä»¬ä¼šåœ¨é¢„å¤„ç†æ—¶å»é™¤ seasonalityï¼Œè®©åç»­æ¨¡å‹æ›´å®¹æ˜“æ£€æµ‹çœŸæ­£çš„å¼‚å¸¸ã€‚
{% endstep %}

{% step %}
### 2. Detection Modelï¼ˆæ£€æµ‹æ¨¡å‹ï¼‰

**English:**\
The detection model is the core. For a fast and interpretable baseline, I could use forecasting models like ARIMA or Prophet. They predict expected latency, and anomalies are large residuals. For more complex scenarios, I might use machine learning like Isolation Forest, or deep learning models such as LSTM Autoencoders, which learn and highlight deviations. The choice depends on accuracy requirements and scalability.

**ä¸­æ–‡:**\
æ£€æµ‹æ¨¡å‹æ˜¯æ ¸å¿ƒã€‚ä¸ºäº†å¿«é€Ÿä¸”å¯è§£é‡Šçš„åŸºçº¿ï¼Œæˆ‘å¯ä»¥ç”¨ ARIMA æˆ– Prophet è¿™æ ·çš„é¢„æµ‹æ¨¡å‹ï¼Œå®ƒä»¬é¢„æµ‹æœŸæœ›å»¶è¿Ÿï¼Œæ®‹å·®å¤§çš„éƒ¨åˆ†å°±æ˜¯å¼‚å¸¸ã€‚åœ¨æ›´å¤æ‚çš„åœºæ™¯ä¸‹ï¼Œå¯ä»¥ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ¯”å¦‚ Isolation Forestï¼Œæˆ–è€…æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ LSTM è‡ªç¼–ç å™¨ï¼Œå»å­¦ä¹ æ­£å¸¸çš„æ—¶é—´æ¨¡å¼å¹¶è¯†åˆ«åå·®ã€‚é€‰æ‹©å“ªç§æ–¹æ³•å–å†³äºç²¾åº¦éœ€æ±‚å’Œå¯æ‰©å±•æ€§ã€‚

* æ–¹æ³•é€‰å‹ï¼ˆå–å†³äºä¸šåŠ¡å’Œæ•°æ®ç‰¹æ€§ï¼‰ï¼š
  * ç»Ÿè®¡æ–¹æ³•ï¼šARIMA, STL decomposition, z-score thresholdã€‚
  * æœºå™¨å­¦ä¹ ï¼šIsolation Forest, One-Class SVMã€‚
  * æ·±åº¦å­¦ä¹ ï¼šLSTM Autoencoder, Transformer-based modelsã€‚
* é¢è¯•ç¤ºä¾‹ï¼šå¦‚æœæˆ‘ä»¬æƒ³å¿«é€Ÿä¸Šçº¿ï¼Œå¯ä»¥ç”¨åŸºäºé¢„æµ‹æ®‹å·®çš„ç»Ÿè®¡æ–¹æ³•ï¼›å¦‚æœè¦ scale å’Œåº”å¯¹æ›´å¤æ‚æ¨¡å¼ï¼Œå¯ä»¥ç”¨ LSTM autoencoderã€‚
{% endstep %}

{% step %}
### 3. Scoringï¼ˆæ‰“åˆ†æœºåˆ¶ï¼‰

**English:**\
Next, we translate model outputs into anomaly scores. For example, residual size or reconstruction error. Then we set thresholds. A static threshold like three standard deviations can work in stable environments, but for latency, a dynamic threshold such as rolling quantiles is more robust because traffic patterns vary over time.

**ä¸­æ–‡:**\
æ¥ä¸‹æ¥ï¼Œéœ€è¦æŠŠæ¨¡å‹è¾“å‡ºè½¬åŒ–ä¸ºå¼‚å¸¸åˆ†æ•°ï¼Œæ¯”å¦‚é¢„æµ‹æ®‹å·®çš„å¤§å°ï¼Œæˆ–è€…è‡ªç¼–ç å™¨çš„é‡å»ºè¯¯å·®ã€‚ç„¶åè®¾å®šé˜ˆå€¼ã€‚åœ¨ç¨³å®šç¯å¢ƒé‡Œï¼Œå¯ä»¥ç”¨å›ºå®šé˜ˆå€¼ï¼ˆæ¯”å¦‚ 3 ä¸ªæ ‡å‡†å·®ï¼‰ï¼›ä½†å¯¹äºå»¶è¿Ÿè¿™ç§éšæ—¶é—´å˜åŒ–çš„ä¿¡å·ï¼Œæ›´åˆé€‚çš„æ˜¯åŠ¨æ€é˜ˆå€¼ï¼Œæ¯”å¦‚æ»šåŠ¨åˆ†ä½æ•°ï¼Œå› ä¸ºæµé‡æ¨¡å¼ä¼šä¸æ–­å˜åŒ–ã€‚

* æ¨¡å‹è¾“å‡º anomaly scoreï¼ˆå¦‚æ®‹å·®ã€é‡å»ºè¯¯å·®ã€æ¦‚ç‡ï¼‰ã€‚
* å°† score è½¬æ¢æˆ interpretable æŒ‡æ ‡ã€‚
* æ–¹æ³•ï¼š
  * å›ºå®šé˜ˆå€¼ï¼ˆæ¯”å¦‚ > 3Ïƒï¼‰ã€‚
  * åŠ¨æ€é˜ˆå€¼ï¼ˆrolling window quantileï¼‰ã€‚
* é¢è¯•ç¤ºä¾‹ï¼šå¯¹äº latency è¿™ç§æŒ‡æ ‡ï¼Œæˆ‘ä¼šç”¨ rolling quantile æ–¹æ³•æ¥åŠ¨æ€è®¾ç½®é˜ˆå€¼ï¼Œå› ä¸ºç³»ç»Ÿè´Ÿè½½åœ¨ä¸åŒæ—¶æ®µå·®å¼‚å¾ˆå¤§ã€‚
{% endstep %}

{% step %}
### 4. Post-processingï¼ˆåå¤„ç† / æŠ¥è­¦ç­–ç•¥ï¼‰

**English:**\
Finally, we refine the alerts to reduce false positives. Instead of alerting on every single anomaly point, I would aggregate anomalies into windows and require multiple consecutive detections before triggering. I would also integrate business rules, such as only alerting if the p95 latency exceeds the SLA threshold. This makes alerts fewer but more actionable for engineers.

**ä¸­æ–‡:**\
æœ€åä¸€æ­¥æ˜¯åå¤„ç†ï¼Œç”¨æ¥å‡å°‘è¯¯æŠ¥ã€‚ä¸æ˜¯æ¯ä¸ªç‚¹å¼‚å¸¸å°±ç«‹åˆ»æŠ¥è­¦ï¼Œè€Œæ˜¯æŠŠå¼‚å¸¸èšåˆæˆæ—¶é—´çª—å£ï¼Œå¹¶è¦æ±‚è¿ç»­å¤šä¸ªç‚¹å¼‚å¸¸æ‰è§¦å‘æŠ¥è­¦ã€‚åŒæ—¶ç»“åˆä¸šåŠ¡è§„åˆ™ï¼Œæ¯”å¦‚åªæœ‰å½“ p95 å»¶è¿Ÿè¶…è¿‡ SLA é˜ˆå€¼æ—¶æ‰æŠ¥è­¦ã€‚è¿™æ ·æŠ¥è­¦æ•°é‡æ›´å°‘ï¼Œä½†æ›´æœ‰æ„ä¹‰ï¼Œå·¥ç¨‹å¸ˆæ‰ä¼šçœŸæ­£å…³æ³¨ã€‚

* é¿å…è¯¯æŠ¥ & æå‡å¯ç”¨æ€§ï¼š
  * è¿ç»­å¤šä¸ªç‚¹å¼‚å¸¸æ‰è§¦å‘å‘Šè­¦ (debouncing)ã€‚
  * èšåˆæˆ anomaly windows è€Œä¸æ˜¯å•ç‚¹æŠ¥è­¦ã€‚
  * ç»“åˆä¸šåŠ¡è§„åˆ™ï¼ˆå¦‚åªåœ¨ p95 > SLA æ—¶æŠ¥è­¦ï¼‰ã€‚
* é¢è¯•ç¤ºä¾‹ï¼šä¸ºäº†é¿å… alert fatigueï¼Œæˆ‘ä»¬ä¼šåŠ  post-processingï¼Œæ¯”å¦‚åªæœ‰å½“è¿ç»­ 3 ä¸ªæ—¶é—´ç‚¹è¶…è¿‡é˜ˆå€¼æ‰å‘å‡ºå‘Šè­¦ã€‚
{% endstep %}
{% endstepper %}

***

### æ”¶å°¾æ€»ç»“

**English:**\
So in summary, my pipeline is: clean and normalize the latency series, apply forecasting or learning-based models, generate anomaly scores with adaptive thresholds, and post-process the results into meaningful alerts. This structure is both systematic and practical for production monitoring systems.

**ä¸­æ–‡:**\
æ€»ç»“æ¥è¯´ï¼Œæˆ‘çš„æµæ°´çº¿å°±æ˜¯ï¼šæ¸…ç†å¹¶æ ‡å‡†åŒ–å»¶è¿Ÿæ•°æ®ï¼Œåº”ç”¨é¢„æµ‹æˆ–å­¦ä¹ æ¨¡å‹ï¼ŒåŸºäºè‡ªé€‚åº”é˜ˆå€¼ç”Ÿæˆå¼‚å¸¸åˆ†æ•°ï¼Œå¹¶é€šè¿‡åå¤„ç†æŠŠç»“æœè½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„å‘Šè­¦ã€‚è¿™æ ·çš„è®¾è®¡æ—¢ç³»ç»ŸåŒ–ï¼Œåˆèƒ½çœŸæ­£ç”¨äºç”Ÿäº§ç›‘æ§ã€‚

***

## Model

### ğŸ“Œ Shared anomaly scoring / é€šç”¨å¼‚å¸¸æ‰“åˆ†

Idea / å‡è®¾

*   EN: Independent of model type (ETS, ARIMA, LSTM, HTMâ€¦), we usually evaluate residuals:

    $$
    s_t = \frac{|y_t - \hat{y}_t|}{\hat{\sigma}_t}
    $$

    where $\hat{\sigma}\_t$ is model-estimated or empirically estimated uncertainty.
*   CN: ä¸è®ºä½¿ç”¨ ETSã€ARIMAã€LSTMã€HTMï¼Œæ ¸å¿ƒæ€è·¯éƒ½æ˜¯åŸºäºæ®‹å·®æ ‡å‡†åŒ–ï¼š

    $$
    s_t = \frac{|y_t - \hat{y}_t|}{\hat{\sigma}_t}
    $$

    å…¶ä¸­ $\hat{\sigma}\_t$ æ¥è‡ªæ¨¡å‹é¢„æµ‹åŒºé—´ï¼Œæˆ–æ»šåŠ¨æ®‹å·®æ–¹å·®ã€‚

How to use / å¦‚ä½•ç”¨äºå¼‚å¸¸æ£€æµ‹

* EN:
  * Prefer model-based PI (prediction interval) if available (ETS, ARIMA, quantile-LSTM).
  * If no PI, use rolling residual variance (EWM std over recent window).
  * Flag when $s\_t$ exceeds threshold (e.g., 99.5% quantile).
  * Add persistence rule (e.g., require 2 out of 3 recent points above threshold) to reduce false alarms.
* CN:
  * æœ‰é¢„æµ‹åŒºé—´çš„æ¨¡å‹ï¼ˆETS/ARIMA/LSTM-Quantileï¼‰â†’ ç”¨æ¨¡å‹ä¸ç¡®å®šåº¦ã€‚
  * æ— é¢„æµ‹åŒºé—´çš„æ¨¡å‹ â†’ ç”¨æ»šåŠ¨æ®‹å·®æ–¹å·®è¿‘ä¼¼ã€‚
  * å¼‚å¸¸åˆ¤å®šï¼š$s\_t$ è¶…è¿‡æ ¡å‡†é˜ˆå€¼ï¼ˆå¦‚ 99.5% åˆ†ä½æ•°ï¼‰ã€‚
  * å¼•å…¥æŒç»­æ€§è§„åˆ™ï¼ˆå¦‚æœ€è¿‘ 3 ç‚¹ä¸­è‡³å°‘ 2 ç‚¹å¼‚å¸¸ï¼‰é™ä½å™ªå£°ã€‚

Variants / æ‰©å±•æ–¹æ³•

* EN:
  * Dynamic thresholds: exponentially weighted moving average (EWMA) of residual distribution.
  * Robust scores: Median Absolute Deviation (MAD) instead of std.
  * One-sided scoring: For latency SLOs, only flag upward deviations (long-tail).
* CN:
  * åŠ¨æ€é˜ˆå€¼ï¼šåŸºäºæ®‹å·®åˆ†å¸ƒçš„æŒ‡æ•°åŠ æƒç§»åŠ¨æ›´æ–°ã€‚
  * ç¨³å¥ç»Ÿè®¡ï¼šç”¨ MADï¼ˆä¸­ä½æ•°ç»å¯¹åå·®ï¼‰ä»£æ›¿æ–¹å·®ã€‚
  * å•ä¾§åˆ¤å®šï¼šå»¶è¿Ÿç±»æŒ‡æ ‡åªå…³æ³¨ **å‘ä¸Šå¼‚å¸¸**ï¼ˆé•¿å°¾é£é™©ï¼‰ã€‚

Pros / Cons

* âœ… Universal: works with any forecaster.
* âœ… Interpretable: standardized score directly shows how abnormal the point is.
* âŒ Needs good variance estimate; otherwise false alarms.
* âŒ May lag in regime shifts (variance baseline needs time to adjust).

***

### Forecasting-based Methods (selected)

#### Exponential Smoothing (ETS / Holt-Winters)

* Idea: Time series decomposed into level (â„“), trend (b), and seasonality (s). Seasonality can be additive or multiplicative. Multiplicative is useful for latency (variance grows with mean).
* Use for anomalies: ETS state-space formulation provides forecasts and prediction intervals. If y\_t âˆ‰ \[L\_t, U\_t], mark anomaly.
* Key knobs: smoothing params Î±, Î², Î³; season length m; additive vs multiplicative; Box-Cox Î» transform.
* Pros: Fast & interpretable; handles single strong seasonality.
* Cons: Limited with multi-seasonality; no natural support for covariates unless extended.

Prod tips: use multiplicative seasonality for latency, log-transform latency, auto-tune via AIC/BIC, re-fit nightly and warm-start online updates.

#### ARIMA / SARIMA / ARIMAX

* Idea: After differencing, model as AR + MA; SARIMA adds seasonal terms; ARIMAX adds exogenous regressors.
* Use for anomalies: produce forecasts with variance estimates; flag based on standardized residuals or prediction intervals.
* Key knobs: orders (p,d,q), seasonal (P,D,Q,m), stationarity tests, exogenous regressors, transforms (log/Box-Cox).
* Pros: Good for linear seasonal signals; principled forecast intervals; supports regressors.
* Cons: Weak for nonlinear dynamics; model selection costly at scale.

Prod tips: use auto-ARIMA for initial search, maintain small per-service models, re-diagnose weekly.

#### LSTM / GRU

* Idea: LSTM uses gates to model long-term dependencies; GRU is a lighter alternative.
* Use for anomalies: train as forecaster (point or quantile); anomalies when actual beyond PI; estimate uncertainty with MC Dropout or ensembles.
* Key knobs: window length, hidden units, dropout, loss (MSE or quantile/pinball), teacher forcing for multi-step.
* Pros: capture nonlinearities and covariates; Cons: data hungry, expensive to train, black-box.

Prod tips: scale latency (log+robust scaler), add exogenous features, quantile regression head for p95/p99, early stopping + drift detection.

#### Echo State Networks (ESN)

* Idea: Reservoir computing â€” fix random recurrent network; only train linear readout.
* Use for anomalies: fast forecaster; anomalies via residuals.
* Pros: Extremely fast training; online adaptation feasible.
* Cons: Sensitive to hyperparams; fewer libraries.

Prod tips: small reservoirs per endpoint, ridge-regularize readout, random re-initialization + validation.

#### Hierarchical Temporal Memory (HTM)

* Idea: Sparse Distributed Representations, online sequence learning; outputs anomaly likelihood directly.
* Use for anomalies: strong for sudden changes/new regimes; outputs anomaly likelihood without explicit forecasting.
* Pros: online learning, adapts to drift automatically.
* Cons: encoder tuning hard; smaller ecosystem.

Prod tips: encode latency in log-scale, add time-of-day/week encoding, use persistence/cooldown to filter transients; pair with ARIMA/LSTM for hybrid.

#### Quantile Forecasting for p95/p99

* Train forecasters to directly predict quantiles (ETS+bootstrapping, Gradient Boosted Quantile, LSTM/GRU with pinball loss). Anomaly if actual exceeds predicted quantile + Î´.

#### Multi-scale & ensembling

* Train at multiple granularities (1-min / 5-min / 1-hour) and combine scores (max standardized residuals or calibrated probability average). Or ensemble multiple model types (ETS/SARIMA/GRU) for robustness.

***

#### Minimal anomaly pipeline (pseudo)

```python
# y_t: log-latency metric (per minute); X_t: covariates (traffic, deploy flags, hour, dow)
model.fit(train_y, X_train)        # ETS/SARIMA/GRU/ESN/HTM (HTM â€œfitâ€ is online)
yhat, sigma = model.predict(y_val, X_val, return_std=True)  # or quantiles

res = np.abs(y_val - yhat)
s = res / (sigma + 1e-6)           # standardized residual
thr = np.quantile(s_calib, 0.995)  # calibrated on clean window
flags = ewma(s > thr, alpha=0.3).astype(int)                # persistence smoothing
alerts = require_n_of_m(flags, n=2, m=3)                    # reduce flaps
```

Production notes:

* Transform: latency â†’ log / Box-Cox; handle zeros with Îµ.
* Calendar: hour-of-day, day-of-week, holidays, deploy flags as regressors.
* Drift: nightly re-fit + weekly hyper-scan; retrain quickly after major releases.
* Cold start: begin with ETS/ESN; backfill training as data accrues.
* Uncertainty: prefer model PI/quantiles over raw residual std.
* Alert hygiene: persistence, cooldown, dedup, severity tiers (mild/warn/critical).
* Evaluation: PR-AUC, F1@K, false-alert-minutes, MAPE/MASE for forecast sanity; replay past incidents as offline backtests.

***

## Datadog Toto æ¨¡å‹ä»‹ç»

**Datadog è‡ªç ”çš„æ—¶åºåŸºç¡€æ¨¡å‹ï¼ˆTime Series Foundation Modelï¼‰**

### 1. ä»€ä¹ˆæ˜¯ Totoï¼Ÿ

* ä¸­æ–‡ï¼š
  * Totoï¼ˆTime Series Optimized Transformer for Observabilityï¼‰æ˜¯ Datadog å¼€å‘çš„æ—¶åºåŸºç¡€æ¨¡å‹ï¼Œä¸“é—¨ä¸ºå¯è§‚æµ‹æ€§æŒ‡æ ‡è®¾è®¡ã€‚
  * å®ƒæ˜¯ä¸€ä¸ªé›¶æ ·æœ¬é¢„æµ‹ï¼ˆzero-shot forecastingï¼‰æ¨¡å‹ï¼Œä¸éœ€è¦ä¸ºæ¯ä¸ªæ–°æŒ‡æ ‡é‡æ–°è®­ç»ƒã€‚
  * ä¸é€šç”¨ LLM ä¸åŒï¼ŒToto é’ˆå¯¹é«˜é¢‘ã€å¤šç»´ã€éå¹³ç¨³ã€æœ‰å¼‚å¸¸ç‚¹çš„ç›‘æ§æŒ‡æ ‡ä¼˜åŒ–ã€‚
* English:
  * Toto (Time Series Optimized Transformer for Observability) is a time series foundation model developed by Datadog.
  * Designed for observability metrics (latency, CPU, traffic, etc.).
  * Enables zero-shot forecasting on new metrics without per-metric retraining.

### 2. è®­ç»ƒæ•°æ®è§„æ¨¡

* ä¸­æ–‡ï¼šåœ¨è¶…è¿‡ 1â€“2 ä¸‡äº¿ä¸ªæ—¶åºæ•°æ®ç‚¹ä¸Šè®­ç»ƒã€‚æ•°æ®æ¥æºï¼šçº¦ 75% æ¥è‡ª Datadog å¹³å°çš„åŒ¿åç›‘æ§æŒ‡æ ‡ï¼Œå…¶ä½™æ¥è‡ªå…¬å¼€æ•°æ®é›†å’Œåˆæˆæ•°æ®ã€‚
* English: Trained on 1â€“2 trillion time series data points. Sources: \~75% anonymized Datadog metrics, plus public datasets and synthetic data.

### 3. æŠ€æœ¯æ¶æ„äº®ç‚¹

* Decoder-only Transformer.
* Proportional Factorized Space-Time Attention for efficient multivariate modeling.
* Student-T mixture prediction head to handle heavy-tailed distributions.
* Patch-based normalization & robust composite loss for non-stationary stability.

### 4. æ€§èƒ½è¡¨ç°

* åœ¨ Datadog å‘å¸ƒçš„ BOOM åŸºå‡†ï¼ˆBenchmark of Observability Metricsï¼‰ä¸Šè¡¨ç°é¢†å…ˆã€‚
* åœ¨ GIFT-Evalã€LSF ç­‰å…¬å¼€æ—¶åºåŸºå‡†æµ‹è¯•ä¸­é›¶æ ·æœ¬æ€§èƒ½ä¼˜å¼‚ã€‚
* åœ¨çœŸå®ç›‘æ§æ•°æ®ä¸Šçš„ sMAPE / sMdAPE æŒ‡æ ‡ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚

***
