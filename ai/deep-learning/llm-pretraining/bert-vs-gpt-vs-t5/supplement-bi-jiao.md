# Supplement比较

以下對 T5、BERT 和 GPT 模型在各個維度進行更深入的比較：

#### 1. 標記化和詞彙

* **BERT**：使用 WordPiece 標記化，詞彙量約 30,000 個標記。
* **GPT**：採用具有大詞彙量的位元組對編碼（BPE）（例如，GPT-3 的詞彙量為175,000）。
* **T5**：利用 SentencePiece 標記化，將文本視為原始文本，不需要預先分段的單字。

#### 2. 預訓練目標

* **BERT**：掩碼語言模型（MLM）和下一句預測（NSP）。
* **GPT**：因果語言模型（CLM），其中每個標記預測序列中的下一個標記。
* **T5**：使用去噪目標，其中隨機的文字範圍被哨兵標記替換，並且模型學習重建原始文字。

#### 3. 輸入表示

* **BERT**：將 Token、Segment 和 Positional Embedding 組合起來表示輸入。
* **GPT**：令牌和位置嵌入相結合（沒有段嵌入，因為它不是為句子對任務設計的）。
* **T5**：僅在註意操作期間新增相對位置編碼的令牌嵌入。

#### 4.注意力機制

* **BERT**：使用絕對位置編碼並允許每個標記關注左側和右側的所有標記（雙向關注）。
* **GPT**：也使用絕對位置編碼，但僅限制對先前標記的注意力（單向注意力）。
* **T5**：實現變壓器的變體，它使用相對位置偏差而不是位置嵌入。

#### 5. 模型架構

* **BERT**：具有多層變壓器區塊的純編碼器架構。
* **GPT**：僅解碼器架構，也具有多層，但專為生成任務而設計。
* **T5**：編碼器-解碼器架構，其中編碼器和解碼器均由變壓器層組成。

#### 6. 微調方法

* **BERT**：根據需要使用附加輸出層調整下游任務的預訓練模型的最終隱藏狀態。
* **GPT**：在變壓器頂部添加一個線性層，並使用相同的因果語言建模目標對下游任務進行微調。
* **T5**：將所有任務轉換為文字到文字格式，其中模型經過微調以從輸入序列產生目標序列。

#### 7. 訓練資料和規模

* **BERT**：接受 BooksCorpus 和英語維基百科的訓練。
* **GPT**：GPT-2 和 GPT-3 已經在從互聯網上提取的不同資料集上進行了訓練，GPT-3 則在一個更大的名為 Common Crawl 的語料庫上進行了訓練。
* **T5**：在「Colossal Clean Crawled Corpus」上進行訓練，這是 Common Crawl 的大型且乾淨的版本。

#### 8. 上下文和雙向性的處理

* **BERT**：旨在同時理解兩個方向的上下文。
* **GPT**：經過訓練可以向前（從左到右）理解上下文。
* **T5**：可以在編碼器中建模雙向上下文，在解碼器中建模單向上下文，適用於序列到序列任務。

#### 9. 下游任務的適應性

* **BERT**：需要特定於任務的頭層並針對每個下游任務進行微調。
* **GPT**：本質上是生成性的，可以在對其結構進行最小改變的情況下提示執行任務。
* **T5**：將每個任務視為「文字到文字」問題，使其具有固有的靈活性並適應新任務。

#### 10. 可解釋性和可解釋性

* **BERT**：雙向性質提供了豐富的上下文嵌入，但可能更難解釋。
* **GPT**：單向上下文可能更容易理解，但缺乏雙向上下文的深度。
* **T5**：編碼器-解碼器框架提供了清晰的處理步驟分離，但由於其生成性質，分析起來可能很複雜。





Reference

* [https://www.unite.ai/zh-TW/nlprise-with-Transformer%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90t5-bert%E5%92%8Cgpt/](https://www.unite.ai/zh-TW/nlprise-with-Transformer%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90t5-bert%E5%92%8Cgpt/)
