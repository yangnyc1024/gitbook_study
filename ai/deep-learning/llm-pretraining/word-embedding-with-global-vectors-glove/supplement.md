# Supplement

## 问GloVe和n-gram word embedding是隶属的关系吗？

GloVe 和 n-gram word embedding 并不是隶属关系，而是两种不同的词嵌入方法，尽管它们都用于将词汇表示为稠密向量。

#### GloVe（Global Vectors for Word Representation）

GloVe 是一种**全局共现矩阵**的词嵌入方法，基于整个语料库中词与词共现的统计信息。它通过构建词与词共现的矩阵，并利用这些统计信息进行词向量学习。具体来说，GloVe 方法通过最小化词向量的损失函数来捕捉词与词之间的共现关系。这种方法可以通过全局语料库生成稠密的词向量，保持语义关系的良好特性。

GloVe 的主要思想是，词与词之间的共现概率能够揭示它们的语义关系。如果两个词在大量上下文中共现，它们的词向量距离会较近。因此，GloVe 注重全局统计特性，不依赖局部上下文。

#### n-gram Word Embedding

n-gram 是一种**基于局部上下文**的词嵌入方法，主要捕捉短距离的词序列（n-gram 是指长度为 n 的词序列，n 可以是 1、2、3 等）。在语言建模或词嵌入中，n-gram 主要用于捕捉词与其上下文之间的依赖关系。

n-gram 嵌入方法可能会通过考虑 n 个词的组合来生成嵌入向量，特别适用于局部上下文建模。它常见于 Skip-gram 或 Continuous Bag of Words (CBOW) 等模型中，这些模型是基于上下文窗口来预测中心词或上下文词的。

#### 区别

* **GloVe** 基于全局共现矩阵，利用整个语料库的词共现信息生成词嵌入。
* **n-gram Word Embedding** 通常是基于局部上下文窗口，通过学习 n-gram 词序列的局部依赖关系来生成嵌入。

因此，二者并不是隶属关系，而是从不同角度建模词汇的语义关系。
