# Word Embedding(word2vec)

自然语言是用来表达人脑思维的复杂系统。在这个系统中，<mark style="color:blue;">词</mark>是意义的基本单元。

* 顾名思义，词向量是用于表示单词意义的向量
* 并且还可以被认为是单词的特征向量或表示

将单词映射到实向量的技术称为词嵌入（word2vec）



## 为何one-hot encoding是一个糟糕的选择

* 我们使用one-hot encoding来表示词（字符就是单词）。假设词典中不同词的数量（词典大小）为 $$N$$，每个词对应一个从 $$0$$ 到 $$N - 1$$ 的不同整数（索引）。
* 为了得到索引为 $$i$$ 的任意词的独热向量表示，我们创建了一个全为 $$0$$ 的长度为 $$N$$ 的向量，并将位置 (i) 的元素设置为 $$1$$。这样，每个词都被表示为一个长度为 $$N$$ 的向量，可以直接由神经网络使用。
* 虽然独热向量很容易构建，但它们通常不是一个好的选择。一个主要原因是独热向量不能准确表达不同词之间的相似度，比如我们经常使用的“余弦相似度”。对于向量 ($$\mathbf{x}, \mathbf{y} \in \mathbb{R}^d$$)，它们的余弦相似度是它们之间角度的余弦：

&#x20;                                                                                      $$\frac{\mathbf{x}^\top \mathbf{y}}{|\mathbf{x}| |\mathbf{y}|} \in [-1, 1]$$

* 由于任意两个不同词的独热向量之间的余弦相似度为 $$0$$，所以one-hot encoding不能编码词之间的相似性。

## 自监督的word2vec

* word2vec工具是为了解决上述问题而提出的。它将每个词映射到一个固定长度的向量，这些向量能更好地表达不同词之间的相似性和类比关系。
* word2vec工具包含两个模型，即跳元模型（skip-gram） (Mikolov et al., 2013)和连续词袋（CBOW） (Mikolov et al., 2013)。
* 对于在语义上有意义的表示，它们的训练依赖于条件概率，条件概率可以被看作使用语料库中一些词来预测另一些单词。由于是不带标签的数据，因此跳元模型和连续词袋都是自监督模型。

## 跳元模型（Skip-Gram）

