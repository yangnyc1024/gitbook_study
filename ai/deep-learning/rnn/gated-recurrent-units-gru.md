# Gated Recurrent Units(GRU)

Mathematically, for a given time step $$ğ‘¡$$, suppose that the input is a minibatch $$ğ‘‹ğ‘¡âˆˆğ‘…ğ‘›Ã—ğ‘‘$$ (number of examples $$=ğ‘›$$; number of inputs $$=ğ‘‘$$) and the hidden state of the previous time step is $$ğ»ğ‘¡âˆ’1âˆˆğ‘…ğ‘›Ã—â„$$ (number of hidden units $$=â„$$). Then the reset gate $$ğ‘…ğ‘¡âˆˆğ‘…ğ‘›Ã—â„$$ and update gate $$ğ‘ğ‘¡âˆˆğ‘…ğ‘›Ã—â„$$ are computed as follows:

$$ğ‘…ğ‘¡=ğœ(ğ‘‹ğ‘¡ğ‘Šxr+ğ»ğ‘¡âˆ’1ğ‘Šhr+ğ‘r),ğ‘ğ‘¡=ğœ(ğ‘‹ğ‘¡ğ‘Šxz+ğ»ğ‘¡âˆ’1ğ‘Šhz+ğ‘z),$$



where and $$ğ‘Šhr,ğ‘Šhzâˆˆğ‘…â„Ã—â„$$ are weight parameters and $$ğ‘r,ğ‘zâˆˆğ‘…1Ã—â„$$ are bias parameters.

#### Conclusion

Compared with LSTMs, GRUs achieve similar performance but tend to be lighter computationally. Generally, compared with simple RNNs, gated RNNS, just like LSTMs and GRUs, can better capture dependencies for sequences with large time step distances. GRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. They can also skip subsequences by turning on the update gate.
