# Gated Recurrent Units(GRU)



## Gated Recurrent Units (GRU) | é—¨æ§å¾ªç¯å•å…ƒ (GRU)

#### 1. **What is GRU? | ä»€ä¹ˆæ˜¯ GRUï¼Ÿ**

**Gated Recurrent Units (GRU)** is a type of recurrent neural network (RNN) architecture introduced by Kyunghyun Cho and colleagues in 2014. It is an improved version of the traditional RNN, designed to solve issues like **vanishing gradients** and to maintain information over longer sequences. GRU is simpler and computationally more efficient than the Long Short-Term Memory (LSTM) network, as it has fewer gates and parameters, making it a popular choice for sequential data tasks such as natural language processing and time series analysis.\
**é—¨æ§å¾ªç¯å•å…ƒ (GRU)** æ˜¯ä¸€ç§é€’å½’ç¥ç»ç½‘ç»œ (RNN) æ¶æ„ï¼Œç”± Kyunghyun Cho ç­‰äººåœ¨ 2014 å¹´æå‡ºã€‚å®ƒæ˜¯å¯¹ä¼ ç»Ÿ RNN çš„æ”¹è¿›ï¼Œæ—¨åœ¨è§£å†³è¯¸å¦‚ **æ¢¯åº¦æ¶ˆå¤±** ç­‰é—®é¢˜ï¼Œå¹¶ä¿æŒé•¿æ—¶é—´åºåˆ—ä¸­çš„ä¿¡æ¯ã€‚GRU æ¯”é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM) æ›´ç®€å•ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ï¼Œå› ä¸ºå®ƒå…·æœ‰æ›´å°‘çš„é—¨æ§æœºåˆ¶å’Œå‚æ•°ï¼Œè¿™ä½¿å¾— GRU åœ¨å¤„ç†è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ—¶é—´åºåˆ—åˆ†æç­‰é¡ºåºæ•°æ®ä»»åŠ¡æ—¶éå¸¸æµè¡Œã€‚

#### 2. **How Does GRU Work? | GRU æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ**

GRU introduces two key gates:

* **Reset Gate**: Determines how much of the past information should be forgotten.
* **Update Gate**: Controls how much of the new information should be added to the current state and how much of the previous state should be retained.

Unlike LSTM, GRU combines the forget and input gates into a single **update gate**, which simplifies the architecture and reduces the number of parameters. The GRU cell dynamically decides which information to keep or discard, making it more efficient in modeling long sequences.\
GRU å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„é—¨æ§ï¼š

* **é‡ç½®é—¨**ï¼šå†³å®šåº”è¯¥é—å¿˜å¤šå°‘è¿‡å»çš„ä¿¡æ¯ã€‚
* **æ›´æ–°é—¨**ï¼šæ§åˆ¶åº”è¯¥å‘å½“å‰çŠ¶æ€æ·»åŠ å¤šå°‘æ–°ä¿¡æ¯ï¼Œä»¥åŠåº”è¯¥ä¿ç•™å¤šå°‘ä¹‹å‰çš„çŠ¶æ€ã€‚

ä¸ LSTM ä¸åŒï¼ŒGRU å°†é—å¿˜é—¨å’Œè¾“å…¥é—¨åˆå¹¶ä¸ºä¸€ä¸ª **æ›´æ–°é—¨**ï¼Œè¿™ç®€åŒ–äº†æ¶æ„å¹¶å‡å°‘äº†å‚æ•°æ•°é‡ã€‚GRU å•å…ƒåŠ¨æ€å†³å®šä¿ç•™æˆ–ä¸¢å¼ƒå“ªäº›ä¿¡æ¯ï¼Œè¿™ä½¿å¾—å®ƒåœ¨å»ºæ¨¡é•¿åºåˆ—æ—¶æ›´åŠ é«˜æ•ˆã€‚

**GRU Equations | GRU æ–¹ç¨‹**

1. **Reset Gate**: Controls how much of the previous hidden state should be forgotten. $$r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$$ where:
   * $$r_t$$ is the reset gate value,
   * $$h_{t-1}$$ is the previous hidden state,
   * $$x_t$$ is the current input,
   * $$W_r$$ are the weights of the reset gate,
   * $$\sigma$$ is the sigmoid function.
2. **Update Gate**: Determines the balance between the previous state and the candidate new state. $$z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$$ where:
   * $$z_t$$ is the update gate value,
   * $$W_z$$ are the weights of the update gate.
3. **Candidate Hidden State**: Computes the new candidate hidden state based on the reset gate. $$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t])$$ where:
   * $$\tilde{h}_t$$ is the candidate hidden state,
   * $$\odot$$ denotes element-wise multiplication.
4. **Final Hidden State**: The final hidden state is a combination of the previous hidden state and the new candidate state, controlled by the update gate. $$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

This mechanism allows the GRU to control the flow of information and effectively maintain long-term dependencies in the data.

1. **é‡ç½®é—¨**ï¼šæ§åˆ¶ä¹‹å‰çš„éšè—çŠ¶æ€æœ‰å¤šå°‘åº”è¯¥è¢«é—å¿˜ã€‚ $$r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$$ å…¶ä¸­ï¼š
   * $$r_t$$ æ˜¯é‡ç½®é—¨å€¼ï¼Œ
   * $$h_{t-1}$$ æ˜¯å‰ä¸€ä¸ªéšè—çŠ¶æ€ï¼Œ
   * $$x_t$$ æ˜¯å½“å‰è¾“å…¥ï¼Œ
   * $$W_r$$ æ˜¯é‡ç½®é—¨çš„æƒé‡ï¼Œ
   * $$\sigma$$ æ˜¯ sigmoid å‡½æ•°ã€‚
2. **æ›´æ–°é—¨**ï¼šå†³å®šä¹‹å‰çš„çŠ¶æ€å’Œå€™é€‰æ–°çŠ¶æ€ä¹‹é—´çš„å¹³è¡¡ã€‚ $$z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$$ å…¶ä¸­ï¼š
   * $$z_t$$ æ˜¯æ›´æ–°é—¨å€¼ï¼Œ
   * $$W_z$$ æ˜¯æ›´æ–°é—¨çš„æƒé‡ã€‚
3. **å€™é€‰éšè—çŠ¶æ€**ï¼šåŸºäºé‡ç½®é—¨è®¡ç®—æ–°çš„å€™é€‰éšè—çŠ¶æ€ã€‚ $$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t])$$ å…¶ä¸­ï¼š
   * $$\tilde{h}_t$$ æ˜¯å€™é€‰éšè—çŠ¶æ€ï¼Œ
   * $$\odot$$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ã€‚
4. **æœ€ç»ˆéšè—çŠ¶æ€**ï¼šæœ€ç»ˆéšè—çŠ¶æ€æ˜¯ä¹‹å‰éšè—çŠ¶æ€å’Œæ–°å€™é€‰çŠ¶æ€çš„ç»„åˆï¼Œç”±æ›´æ–°é—¨æ§åˆ¶ã€‚ $$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

è¿™ç§æœºåˆ¶ä½¿ GRU èƒ½å¤Ÿæ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ï¼Œå¹¶æœ‰æ•ˆä¿æŒæ•°æ®ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚

#### 3. **Comparison to LSTM | ä¸ LSTM çš„æ¯”è¾ƒ**

* **Gates | é—¨æ§**: GRU has two gates (reset and update), while LSTM has three gates (forget, input, and output). This makes GRU simpler and faster to compute. **é—¨æ§**ï¼šGRU æœ‰ä¸¤ä¸ªé—¨ï¼ˆé‡ç½®å’Œæ›´æ–°ï¼‰ï¼Œè€Œ LSTM æœ‰ä¸‰ä¸ªé—¨ï¼ˆé—å¿˜ã€è¾“å…¥å’Œè¾“å‡ºï¼‰ã€‚è¿™ä½¿å¾— GRU æ›´åŠ ç®€å•ï¼Œè®¡ç®—é€Ÿåº¦æ›´å¿«ã€‚
* **Performance | æ€§èƒ½**: GRU often performs similarly to LSTM on many tasks, but it is more computationally efficient due to its simpler structure. **æ€§èƒ½**ï¼šåœ¨è®¸å¤šä»»åŠ¡ä¸­ï¼ŒGRU çš„è¡¨ç°ä¸ LSTM ç±»ä¼¼ï¼Œä½†ç”±äºå…¶ç»“æ„æ›´ç®€å•ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ã€‚
* **Memory Efficiency | å†…å­˜æ•ˆç‡**: GRU uses fewer parameters and less memory than LSTM, making it a good choice for resource-constrained environments. **å†…å­˜æ•ˆç‡**ï¼šGRU ä½¿ç”¨çš„å‚æ•°æ›´å°‘ï¼Œå†…å­˜æ•ˆç‡æ›´é«˜ï¼Œè¿™ä½¿å¾—å®ƒåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

#### 4. **Why Use GRU? | ä¸ºä»€ä¹ˆä½¿ç”¨ GRUï¼Ÿ**

* **Efficient for Long Sequences | é€‚ç”¨äºé•¿åºåˆ—**: GRU is effective at handling long sequences of data, making it suitable for tasks like machine translation, speech recognition, and time series prediction. **é€‚ç”¨äºé•¿åºåˆ—**ï¼šGRU èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿åºåˆ—æ•°æ®ï¼Œé€‚ç”¨äºè¯¸å¦‚æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«å’Œæ—¶é—´åºåˆ—é¢„æµ‹ç­‰ä»»åŠ¡ã€‚
* **Simpler and Faster than LSTM | æ¯” LSTM æ›´ç®€å•å’Œå¿«é€Ÿ**: GRUâ€™s simpler structure with fewer gates makes it faster to train and requires fewer computational resources compared to LSTM. **æ¯” LSTM æ›´ç®€å•å’Œå¿«é€Ÿ**ï¼šGRU çš„ç»“æ„æ›´ç®€å•ï¼Œé—¨æ§æ›´å°‘ï¼Œä½¿å…¶è®­ç»ƒæ›´å¿«ï¼Œæ‰€éœ€è®¡ç®—èµ„æºæ¯” LSTM å°‘ã€‚
* **Prevents Vanishing Gradients | é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±**: Like LSTM, GRU addresses the vanishing gradient problem in traditional RNNs, allowing it to maintain long-term dependencies more effectively. **é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±**ï¼šä¸ LSTM ç±»ä¼¼ï¼ŒGRU è§£å†³äº†ä¼ ç»Ÿ RNN ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¿æŒé•¿æœŸä¾èµ–å…³ç³»ã€‚

#### 5. **Applications of GRU | GRU çš„åº”ç”¨**

* **Natural Language Processing (NLP) | è‡ªç„¶è¯­è¨€å¤„ç†**: GRUs are widely used in tasks such as machine translation, language modeling, and text generation, where maintaining long-term dependencies is crucial. **è‡ªç„¶è¯­è¨€å¤„ç† (NLP)**ï¼šGRU å¹¿æ³›ç”¨äºæœºå™¨ç¿»è¯‘ã€è¯­è¨€å»ºæ¨¡å’Œæ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œä¿æŒé•¿æœŸä¾èµ–å…³ç³»è‡³å…³é‡è¦ã€‚
* **Speech Recognition | è¯­éŸ³è¯†åˆ«**: GRU is often used in speech recognition systems to model sequential audio data and capture long-term dependencies between sounds. **è¯­éŸ³è¯†åˆ«**ï¼šGRU å¸¸ç”¨äºè¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­ï¼Œä»¥å»ºæ¨¡åºåˆ—éŸ³é¢‘æ•°æ®å¹¶æ•æ‰å£°éŸ³ä¹‹é—´çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚
* **Time Series Prediction | æ—¶é—´åºåˆ—é¢„æµ‹**: GRU is effective in forecasting future data points in time series analysis, such as stock prices, weather patterns, or energy consumption. **æ—¶é—´åºåˆ—é¢„æµ‹**ï¼šGRU åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­æœ‰æ•ˆé¢„æµ‹æœªæ¥æ•°æ®ç‚¹ï¼Œå¦‚è‚¡ç¥¨ä»·æ ¼ã€å¤©æ°”æ¨¡å¼æˆ–èƒ½æºæ¶ˆè€—ã€‚

#### 6. **Limitations of GRU | GRU çš„å±€é™æ€§**

* **Not Always Better than LSTM | å¹¶ä¸æ€»æ˜¯ä¼˜äº LSTM**: While GRU is more efficient in some cases, LSTM may outperform GRU on certain complex tasks where long-term dependencies are more critical. **å¹¶ä¸æ€»æ˜¯ä¼˜äº LSTM**ï¼šè™½ç„¶ GRU åœ¨æŸäº›æƒ…å†µä¸‹æ•ˆç‡æ›´é«˜ï¼Œä½†åœ¨ä¸€äº›é•¿æœŸä¾èµ–å…³ç³»æ›´ä¸ºå…³é”®çš„å¤æ‚ä»»åŠ¡ä¸­ï¼ŒLSTM å¯èƒ½è¡¨ç°æ›´å¥½ã€‚
* **Simpler Structure Can Be a Limitation | ç®€åŒ–çš„ç»“æ„å¯èƒ½æˆä¸ºå±€é™**: The reduced number of gates in GRU may limit its ability to control information flow as effectively as LSTM in highly complex tasks. **ç®€åŒ–çš„ç»“æ„å¯èƒ½æˆä¸ºå±€é™**ï¼šGRU ä¸­å‡å°‘çš„é—¨æ§æ•°é‡å¯èƒ½ä¼šé™åˆ¶å…¶åœ¨é«˜åº¦å¤æ‚ä»»åŠ¡ä¸­æœ‰æ•ˆæ§åˆ¶ä¿¡æ¯æµçš„èƒ½åŠ›ã€‚

#### 7. **Example of GRU in Action | GRU çš„æ“ä½œç¤ºä¾‹**

Consider an example where a GRU model is used for **machine translation** from English to French:

1. The GRU receives a sequence of English words as input.
2. As each word is processed, the GRU updates its hidden state based on the previous hidden state and the current word, keeping track of the context.
3. The final hidden state is used to generate the French translation of the sentence.

In this case, GRUâ€™s ability to maintain information over long sequences helps it retain the meaning of the sentence and produce accurate translations.\
ä»¥ GRU æ¨¡å‹ç”¨äº **æœºå™¨ç¿»è¯‘**ï¼ˆä»è‹±è¯­åˆ°æ³•è¯­ï¼‰ä¸ºä¾‹ï¼š

1. GRU æ¥æ”¶ä¸€ç³»åˆ—è‹±è¯­å•è¯ä½œä¸ºè¾“å…¥ã€‚
2. åœ¨å¤„ç†æ¯ä¸ªå•è¯æ—¶ï¼ŒGRU åŸºäºä¹‹å‰çš„éšè—çŠ¶æ€å’Œå½“å‰å•è¯æ›´æ–°å…¶éšè—çŠ¶æ€ï¼Œè·Ÿè¸ªä¸Šä¸‹æ–‡ã€‚
3. æœ€ç»ˆçš„éšè—çŠ¶æ€ç”¨äºç”Ÿæˆå¥å­çš„æ³•è¯­ç¿»è¯‘ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒGRU ä¿æŒé•¿åºåˆ—ä¿¡æ¯çš„èƒ½åŠ›å¸®åŠ©å®ƒä¿ç•™å¥å­çš„å«ä¹‰å¹¶ç”Ÿæˆå‡†ç¡®çš„ç¿»è¯‘ã€‚

## Extra Reading

æˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•åœ¨å¾ªç¯ç¥ç»ç½‘ç»œä¸­è®¡ç®—æ¢¯åº¦ï¼Œ ä»¥åŠçŸ©é˜µè¿ç»­ä¹˜ç§¯å¯ä»¥å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚ ä¸‹é¢æˆ‘ä»¬ç®€å•æ€è€ƒä¸€ä¸‹è¿™ç§æ¢¯åº¦å¼‚å¸¸åœ¨å®è·µä¸­çš„æ„ä¹‰ï¼š

Mathematically, for a given time step $$ğ‘¡$$, suppose that the input is a minibatch $$ğ‘‹_ğ‘¡âˆˆğ‘…^ğ‘›Ã—ğ‘‘$$ (number of examples $$=ğ‘›$$; number of inputs $$=ğ‘‘$$) and the hidden state of the previous time step is $$ğ»ğ‘¡âˆ’1âˆˆğ‘…ğ‘›Ã—â„$$ (number of hidden units $$=â„$$). Then the reset gate $$ğ‘…ğ‘¡âˆˆğ‘…ğ‘›Ã—â„$$ and update gate $$ğ‘_ğ‘¡ \in ğ‘…_ğ‘› \times â„$$ are computed as follows:

$$ğ‘…_ğ‘¡=ğœ(ğ‘‹_ğ‘¡ğ‘Š_xr+ğ»_ğ‘¡âˆ’1ğ‘Š_hr+ğ‘r),ğ‘ğ‘¡=ğœ(ğ‘‹ğ‘¡ğ‘Š_xz+ğ»_{t-1}ğ‘Š_hz+ğ‘z),$$

where and $$ğ‘Š_hr,ğ‘Š_hz \in ğ‘…^â„ \times â„$$ are weight parameters and  $$b_r, b_z \in ğ‘…^1Ã—â„$$ are bias parameters.

#### Conclusion

Compared with LSTMs, GRUs achieve similar performance but tend to be lighter computationally. Generally, compared with simple RNNs, gated RNNS, just like LSTMs and GRUs, can better capture dependencies for sequences with large time step distances. GRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. They can also skip subsequences by turning on the update gate.

**GRU** æ˜¯ LSTM çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é¡ºåºæ•°æ®ï¼ŒåŒæ—¶è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚**é‡ç½®é—¨å’Œæ›´æ–°é—¨** æ§åˆ¶ä¿¡æ¯åœ¨ç½‘ç»œä¸­çš„æµåŠ¨ã€‚GRU çš„ä¼˜åŠ¿åŒ…æ‹¬æ¯” LSTM æ›´ç®€å•å’Œå¿«é€Ÿã€é€‚ç”¨äºé•¿åºåˆ—ã€è®¡ç®—æ•ˆç‡é«˜ã€‚GRU å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œæ—¶é—´åºåˆ—é¢„æµ‹ç­‰é¢†åŸŸã€‚
