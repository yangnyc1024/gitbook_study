# Gated Recurrent Units(GRU)

æˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•åœ¨å¾ªç¯ç¥ç»ç½‘ç»œä¸­è®¡ç®—æ¢¯åº¦ï¼Œ ä»¥åŠçŸ©é˜µè¿ç»­ä¹˜ç§¯å¯ä»¥å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚ ä¸‹é¢æˆ‘ä»¬ç®€å•æ€è€ƒä¸€ä¸‹è¿™ç§æ¢¯åº¦å¼‚å¸¸åœ¨å®è·µä¸­çš„æ„ä¹‰ï¼š

Mathematically, for a given time step $$ğ‘¡$$, suppose that the input is a minibatch $$ğ‘‹_ğ‘¡âˆˆğ‘…^ğ‘›Ã—ğ‘‘$$ (number of examples $$=ğ‘›$$; number of inputs $$=ğ‘‘$$) and the hidden state of the previous time step is $$ğ»ğ‘¡âˆ’1âˆˆğ‘…ğ‘›Ã—â„$$ (number of hidden units $$=â„$$). Then the reset gate $$ğ‘…ğ‘¡âˆˆğ‘…ğ‘›Ã—â„$$ and update gate $$ğ‘_ğ‘¡ \in ğ‘…_ğ‘› \times â„$$ are computed as follows:

$$ğ‘…_ğ‘¡=ğœ(ğ‘‹_ğ‘¡ğ‘Š_xr+ğ»_ğ‘¡âˆ’1ğ‘Š_hr+ğ‘r),ğ‘ğ‘¡=ğœ(ğ‘‹ğ‘¡ğ‘Š_xz+ğ»_{t-1}ğ‘Š_hz+ğ‘z),$$



where and $$ğ‘Š_hr,ğ‘Š_hz \in ğ‘…^â„ \times â„$$ are weight parameters and  $$b_r, b_z \in ğ‘…^1Ã—â„$$ are bias parameters.

#### Conclusion

Compared with LSTMs, GRUs achieve similar performance but tend to be lighter computationally. Generally, compared with simple RNNs, gated RNNS, just like LSTMs and GRUs, can better capture dependencies for sequences with large time step distances. GRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. They can also skip subsequences by turning on the update gate.
