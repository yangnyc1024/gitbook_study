# Seuqence to Sequence Learning(Seq2Seq)

## Introduction

从上一个section里面我们知道，机器翻译的输入和输出序列都是长度可变的，为了解决这个问题，我们使用了decoder-ecoder模型，透过两个循环神经网络的编码器和解码器，透过一个hidden的中间层，将前后两个序列（语言）练习在一起，以至于我们可以用于_序列到序列_（sequence to sequence，seq2seq）类的学习任务

* 遵循decoder-ecoder的设计原则，RNN的编码器使用长度可以改变的序列作为输入，将其转换为固定形状的隐状态
* 为了可以连续生成输出的序列的词元，独立的RNN解码器是基于
  * 输入序列的编码信息
  * 已经看见的或者生成的词元来预测下一个词元
*

    <figure><img src="../../.gitbook/assets/Screenshot 2024-09-12 at 11.43.41 PM.png" alt="" width="563"><figcaption></figcaption></figure>

## Encoder

从技术上讲，编码器将长度可变的输入序列转换成形状固定的上下文变量$$c$$，并且将输入序列的信息在该上下文变量中进行编码, 可以使用循环神经网络来设计编码器:

* 考虑由一个序列组成的样本（批量大小=1）。假设输入序列是$$x_1, \ldots, x_T$$，其中$$x_t$$ 是输入文本序列中的第$$t$$个词元。
* 在时间步$$t$$，循环神经网络将词元$$x_t$$的输入特征向量$$x_t$$和$$h_{t-1}$$（即上一步的隐状态）转换为$$h_t$$（即当前步的隐状态）。使用一个函数$$f$$来描述循环神经网络的循环层所做的变换： $$h_t = f(x_t, h_{t-1})$$
* 总之，编码器通过选定的函数$$q$$，将所有时间步的隐状态转换为上下文变量： $$c = q(h_1, \ldots, h_T)$$比如，当选择$$q(h_1, \ldots, h_T) = h_T$$时，上下文变量仅仅是输入序列在最后时间步的隐状态$$h_T$$。

到目前为止，我们使用的是一个单向循环神经网络设计编码器，其中隐状态只依赖于输入子序列，这个子序列是由输入序列的起始位置到隐状态所在的时间步的位置（包括隐状态所在的时间步）组成。

我们也可以使用双向循环神经网络构造编码器，其中隐状态依赖于两个输入子序列，两个子序列是由隐状态所在的时间步之前的序列和之后的序列（包括隐状态所在的时间步），因此隐状态对整个序列的信息都进行了编码。

现在，让我们实现循环神经网络编码器。

* 注意，我们使用了嵌入层（embedding layer）来获得输入序列中每个词元的特征向量。嵌入层的权重是一个矩阵，其行数等于输入词表的大小（vocab\_size），其列数等于特征向量的维度（embed\_size）。
* 对于任意词元的索引$$i$$，嵌入层获取权重矩阵的第$$i$$行（从0开始）以返回其特征向量。另外，本文选择了一个多层门控循环单元来实现编码器。

## Decoder

正如上文提到的，编码器输出的上下文变量$$c$$对整个输入序列$$x_1, \ldots, x_T$$进行编码。来自训练数据集的输出序列$$y_1, y_2, \ldots, y_T$$，对于每个时间步$$t'$$（与输入序列或编码器的时间步$$t$$不同），解码器输出$$y_{t'}$$的概率取决于先前的输出子序列$$y_1, \ldots, y^{t'-1}$$和上下文变量$$c$$，即$$P(y_{t'} \mid y_1, \ldots, y^{t'-1}, c)$$。

为了在序列上模型化这种条件概率，我们可以使用另一个循环神经网络作为解码器。

* 在输出序列上的任意时间步$t'$，循环神经网络将来自上一步时间步的输出$$y_{t'-1}$$\
  和上下文变量$$c$$作为其输入，然后在当前时间步将它们和上一隐状态$$s_{t'-1}$$转换为隐状态$$s_{t'}$$。因此，可以使用函数$$g$$来表示解码器的隐藏层的变换： $$s_{t'} = g(y_{t'-1}, c, s_{t'-1})$$
* 在获得解码器的隐状态之后，我们可以使用输出层和softmax操作来计算在时间步$t'$时输出$y\_{t'}$的条件概率分布$$P(y_{t'} \mid y_1, \ldots, y^{t'-1}, c)$$。

当实现解码器时，我们直接使用编码器最后一个时间步的隐状态初始化解码器的隐状态。这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。为了进一步包含经过编码的输入序列的信息，上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。
