# The Bahdanau Attention Mechanism



我们之前提到过，透过基于两个RNN的encoder-decoder的架构，用于序列到序列的学习

* RNN可以将长度可变的序列转换为固定形状的上下文变量，然后透过RNN根据生成的词元和上下文变量，按词元生成输出（目标）序列词元
* 然而，即使并非所有的输入（源）词元都对某个解码某个词元都有用，在每个解码步骤中仍然使用编码上下文变量

那有什么方法可以改变上下文变量呢

* Bahdanau等人提出了一个没有严格单向对齐限制的可微注意力模型。
* 在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。
* 这是通过将上下文变量视为注意力集中的输出来实现

## 模型

下面描述的Bahdanau注意力模型将遵循前面encoder-decoder中的相同符号表述。这个新的基于注意力的模型与前面中的模型相同，只不过前文中的上下文变量 $$c$$ 在任何解码时刻$$t'$$都会被 $$c_{t'}$$ 替换。假设输入序列中有 $$T$$ 个词元，解码时刻 $$t'$$ 的上下文变量是注意力集中输出：

$$c_{t'} = \sum_{t=1}^{T} \alpha(s_{t'-1}, h_t) h_t$$

其中，时刻 $$t' - 1$$ 时的解码器隐层状态 $$s_{t'-1}$$ 是查询，编码器隐状态 $$h_t$$ 既是键，也是值，注意力权重 $$\alpha$$ 是使用前面所定义的加性注意力打分函数计算的，以下描述了Bahdanau注意力的架构：

<figure><img src="../../../.gitbook/assets/Screenshot 2024-09-13 at 2.18.44 PM.png" alt="" width="375"><figcaption></figcaption></figure>
