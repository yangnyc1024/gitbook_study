# Residual Networks(ResNet) and ResNeXt

* 只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。 对于深度神经网络，如果我们能将新添加的层训练成_恒等映射_（identity function）$$f(x) = x$$，新模型和原模型将同样有效。 同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。
* 针对这一问题，何恺明等人提出了_残差网络_（ResNet） ([He _et al._, 2016](https://zh.d2l.ai/chapter\_references/zreferences.html#id60))。 它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。
* &#x20;残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 于是，_残差块_（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。 凭借它，ResNet赢得了2015年ImageNet大规模视觉识别挑战赛。

#### Residual Block

* 让我们聚焦于神经网络局部：如图所示，假设我们的原始输入为$$x$$，而希望学出的理想映射为$$f(x)$$
* 左图虚线框中的部分需要直接拟合出该映射$$f(x)$$，而右图虚线框中的部分则需要拟合出残差映射$$f(x) - x$$。 残差映射在现实中往往更容易优化。
* &#x20;以本节开头提到的恒等映射作为我们希望学出的理想映射$$f(x)$$，我们只需将图中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么$$f(x)$$即为恒等映射。
* &#x20;实际中，当理想映射$$f(x)$$极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。 右图是ResNet的基础架构–_残差块_（residual block）。 在残差块中，输入可通过跨层数据线路更快地向前传播。
*

    <figure><img src="../../.gitbook/assets/Screenshot 2024-02-05 at 3.43.41 PM.png" alt="" width="375"><figcaption></figcaption></figure>
* ResNet沿用了VGG完整的$$3×3$$卷积层设计。 残差块里首先有2个有相同输出通道数的$$3×3$$卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。

#### ResNet模型

* ResNet的前两层跟之前介绍的GoogLeNet中的一样： 在输出通道数为64、步幅为2的$$7×7$$卷积层后，接步幅为2的$$3×3$$的最大汇聚层。 不同之处在于ResNet每个卷积层后增加了批量规范化层。
* GoogLeNet在后面接了4个由Inception块组成的模块。 ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。

#### Summary

* 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。
* 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。
* 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。
* 残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。
