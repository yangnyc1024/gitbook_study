# Batch Normalization

* 训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。 本节将介绍_批量规范化_（batch normalization） ([Ioffe and Szegedy, 2015](https://zh.d2l.ai/chapter\_references/zreferences.html#id75))，这是一种流行且有效的技术，可持续加速深层网络的收敛速度。 再结合在 [7.6节](https://zh.d2l.ai/chapter\_convolutional-modern/resnet.html#sec-resnet)中将介绍的残差块，批量规范化使得研究人员能够训练100层以上的网络。

#### 为什么需要批量规范化层呢？

* 首先，数据预处理的方式通常会对最终结果产生巨大影响。
* 第二，对于典型的多层感知机或卷积神经网络。当我们训练时，中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围：不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。 批量规范化的发明者非正式地假设，这些变量分布中的这种偏移可能会阻碍网络的收敛。 直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。
* 第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要。
  * 事实证明，这是深度学习中一个反复出现的主题。 由于尚未在理论上明确的原因，优化中的各种噪声源通常会导致更快的训练和较少的过拟合：这种变化似乎是正则化的一种形式。 在一些初步研究中， ([Teye _et al._, 2018](https://zh.d2l.ai/chapter\_references/zreferences.html#id167))和 ([Luo _et al._, 2018](https://zh.d2l.ai/chapter\_references/zreferences.html#id103))分别将批量规范化的性质与贝叶斯先验相关联。 这些理论揭示了为什么批量规范化最适应$$50∼100$$范围中的中等批量大小的难题。
  * 另外，批量规范化层在”训练模式“（通过小批量统计数据规范化）和“预测模式”（通过数据集统计规范化）中的功能不同。 在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。 而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。

#### 批量规划化层

* 全连接层
* 卷积层
* 预测过程中的批量规范化

#### Discussion

* 直观地说，批量规范化被认为可以使优化更加平滑。 然而，我们必须小心区分直觉和对我们观察到的现象的真实解释。 回想一下，我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。 即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。
* 在提出批量规范化的论文中，作者除了介绍了其应用，还解释了其原理：通过减少_内部协变量偏移_（internal covariate shift）。 据推测，作者所说的_内部协变量转移_类似于上述的投机直觉，即变量值的分布在训练过程中会发生变化。 然而，这种解释有两个问题： 1、这种偏移与严格定义的_协变量偏移_（covariate shift）非常不同，所以这个名字用词不当； 2、这种解释只提供了一种不明确的直觉，但留下了一个有待后续挖掘的问题：为什么这项技术如此有效？ 本书旨在传达实践者用来发展深层神经网络的直觉。 然而，重要的是将这些指导性直觉与既定的科学事实区分开来。 最终，当你掌握了这些方法，并开始撰写自己的研究论文时，你会希望清楚地区分技术和直觉。
* 随着批量规范化的普及，_内部协变量偏移_的解释反复出现在技术文献的辩论，特别是关于“如何展示机器学习研究”的更广泛的讨论中。 Ali Rahimi在接受2017年NeurIPS大会的“接受时间考验奖”（Test of Time Award）时发表了一篇令人难忘的演讲。他将“内部协变量转移”作为焦点，将现代深度学习的实践比作炼金术。 他对该示例进行了详细回顾 ([Lipton and Steinhardt, 2018](https://zh.d2l.ai/chapter\_references/zreferences.html#id97))，概述了机器学习中令人不安的趋势。 此外，一些作者对批量规范化的成功提出了另一种解释：在某些方面，批量规范化的表现出与原始论文 ([Santurkar _et al._, 2018](https://zh.d2l.ai/chapter\_references/zreferences.html#id143))中声称的行为是相反的。
* 然而，与机器学习文献中成千上万类似模糊的说法相比，内部协变量偏移没有更值得批评。 很可能，它作为这些辩论的焦点而产生共鸣，要归功于目标受众对它的广泛认可。 批量规范化已经被证明是一种不可或缺的方法。它适用于几乎所有图像分类器，并在学术界获得了数万引用。
