# Basic

当你设计卷积给图像使用的时候，需要保证一个所谓的不变性

1. _平移不变性_（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
2. _局部性_（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。



#### Padding

假设输入形状为$$n_h \times n_w$$，卷积核形状为$$k_h \times k_w$$，那么输出形状将是$$(n_h -k _h + 1) \times (n_w -k_w +1)$$，

why？

* 在应用多层卷积时，我们常常丢失边缘像素。 由于我们通常使用小卷积核，因此对于任何单个卷积，我们可能只会丢失几个像素。 但随着我们应用许多连续卷积层，累积丢失的像素数就多了。
* 解决这个问题的简单方法即为_填充_（padding）：在输入图像的边界填充元素（通常填充元素是$$0$$）
*   通常，如果我们添加$$p_h$$行填充（大约一半在顶部，一半在底部）和$$p_w$$列填充（左侧大约一半，右侧一半），则输出形状将为

    $$(n_h -k_h + p_h +1) \times (n_w -k_w+ p_w + 1)$$

#### Stride

* 在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。 在前面的例子中，我们默认每次滑动一个元素。 但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。
* 通常，当垂直步幅为$$s_h$$、水平步幅为$$s_w$$时，输出形状为
  * $$[(n_h- k_h + p_h + s_h)/s_h] \times[(n_w -k_w + p_w +s_w) / s_w]$$
  * \[] means 向下取整这里

#### Channel

* 当我们添加通道时，我们的输入和隐藏的表示都变成了三维张量。例如，每个RGB输入图像具有$$3\times h \times w$$的形状。我们将这个大小为$$3$$的轴称为_通道_（channel）维度。本节将更深入地研究具有多输入和多输出通道的卷积核。
* 用$$c_i$$和$$c_0$$分别表示输入和输出通道的数目，并让$$k_h$$和$$k_w$$为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为$$c_i \times k_h \times k_w$$的卷积核张量，这样卷积核的形状是$$c_o \times c_i \times k_h \times k_w$$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。



#### Pooling

* 通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大
* 而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以我们最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。
* 然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。 相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为_最大汇聚层_（maximum pooling）和_平均汇聚层_（average pooling）。
