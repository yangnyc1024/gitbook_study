# Gradient Vanishing vs Gradient Exploding

### 梯度消失与梯度爆炸

在训练深层神经网络时，梯度消失和梯度爆炸是两种常见的问题。它们都与反向传播过程中梯度的更新方式有关，特别是在网络层数较多时尤为显著。

#### 梯度消失（Gradient Vanishing）

梯度消失发生在反向传播过程中，梯度随着层数增加逐渐变得非常小，以至于无法对较早层的权重进行有效更新。这通常发生在使用某些激活函数（如 Sigmoid 或 Tanh）时。

**原因：**

在反向传播中，梯度通过链式法则计算：

$$
\frac{\partial \text{Loss}}{\partial w} = \frac{\partial \text{Loss}}{\partial z_L} \cdot \frac{\partial z_L}{\partial z_{L-1}} \cdot \cdots \cdot \frac{\partial z_1}{\partial w}
$$

当层数 $$L$$ 较大时，每一层的导数如果小于 1，那么梯度会呈指数级减小，最终导致前几层几乎没有梯度更新。

例如，对于 Sigmoid 激活函数：

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

其导数为：

$$
\sigma'(z) = \sigma(z)(1 - \sigma(z))
$$

当 $$\sigma(z)$$ 靠近 0 或 1 时，导数会变得非常小，导致梯度消失。

**影响：**

* 神经网络的早期层权重几乎不会更新，学习变得非常缓慢。
* 随着网络层数的增加，这个问题会变得更加严重。

#### 梯度爆炸（Gradient Exploding）

梯度爆炸是与梯度消失相对的问题。当层数较多时，梯度在反向传播中不断增大，导致权重更新过大，模型变得不稳定。

**原因：**

与梯度消失类似，梯度爆炸也是由于链式法则的累乘。如果某一层的梯度大于 1，则通过多层传递后，梯度会变得非常大，导致参数更新过大。

例如，在网络中使用线性激活函数或 ReLU 激活函数时，如果权重初始化不当或者梯度更新没有限制，容易发生梯度爆炸。

**影响：**

* 模型的权重可能会变得非常大，导致梯度更新不稳定，甚至可能导致损失函数变为 NaN。
* 训练过程可能会失败，网络无法收敛。

#### 例子

假设我们有一个 3 层的神经网络，每一层使用 Sigmoid 作为激活函数。对于输入 $$x$$，我们通过权重 $$w$$ 和偏置 $$b$$ 计算输出 $$\hat{y}$$：

$$
z_1 = w_1 \cdot x + b_1
$$

$$
a_1 = \sigma(z_1)
$$

$$
z_2 = w_2 \cdot a_1 + b_2
$$

$$
a_2 = \sigma(z_2)
$$

$$
z_3 = w_3 \cdot a_2 + b_3
$$

$$
\hat{y} = \sigma(z_3)
$$

在反向传播时，若 Sigmoid 的导数值较小，前几层的权重更新会变得非常缓慢，导致梯度消失。而如果我们初始化权重过大，则可能导致梯度在反向传播中指数级增长，最终发生梯度爆炸。

#### 解决方法：

1. **梯度消失**：
   * 使用不同的激活函数，例如 ReLU、Leaky ReLU 等，它们的导数不会像 Sigmoid 那样趋近于 0。
   * 使用批归一化（Batch Normalization）来缓解梯度消失问题。
   * 使用合适的权重初始化方法（例如 Xavier 初始化或 He 初始化）。
2. **梯度爆炸**：
   * 使用梯度裁剪（Gradient Clipping）来限制梯度的大小。
   * 使用更小的学习率。
   * 使用合适的权重初始化来防止梯度在反向传播时过大。
