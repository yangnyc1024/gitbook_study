---
description: Extra Info
---

# Extra

## **Encoder 和 Decoder 的数学区别**

### **1. 主要区别概览**

| 组件          | 输入                                       | 计算方式                                           | 输出                                     |
| ----------- | ---------------------------------------- | ---------------------------------------------- | -------------------------------------- |
| **Encoder** | 输入序列 $$x_1, x_2, \dots, x_T$$            | 逐步更新隐藏状态 $$h_t = f(h_{t-1}, x_t)$$             | 最后一个隐藏状态 $$h_T$$ 或上下文向量 $$c$$          |
| **Decoder** | 上下文向量 $$c$$ + 目标序列的前一 token $$y_{t'-1}$$ | 逐步更新隐藏状态 $$s_{t'} = g(s_{t'-1}, y_{t'-1}, c)$$ | 生成的 token $$y_{t'}$$，通过 softmax 计算概率分布 |

***

### **2. Encoder（编码器）的数学公式**

#### **2.1 输入**

编码器的输入是一个序列：

$$
x = (x_1, x_2, \dots, x_T)
$$

其中，每个 $$x_t$$ 是输入的 token（例如单词的向量表示）。

#### **2.2 递归计算**

编码器使用递归公式计算隐藏状态：

$$
h_t = f(h_{t-1}, x_t)
$$

其中：

* $$h_t$$ 是时间步 $$t$$ 的隐藏状态。
* $$f$$ 是 RNN/LSTM/GRU 的转换函数。

**最简单的 RNN 计算方式**：

$$
h_t = \tanh(W_h h_{t-1} + W_x x_t + b)
$$

**LSTM 计算方式**（包含遗忘门、输入门、输出门）：

$$
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
$$

$$
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
$$

$$
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
$$

$$
\tilde{c}_t = \tanh(W_c [h_{t-1}, x_t] + b_c)
$$

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

$$
h_t = o_t \odot \tanh(c_t)
$$

最终，编码器的隐藏状态 $$h_T$$ 形成上下文向量 $$c$$：

$$
c = m(h_1, h_2, \dots, h_T)
$$

其中 $$m$$ 可能是直接取最后一个隐藏状态（简单情况）或某种加权平均（如注意力机制）。

***

### **3. Decoder（解码器）的数学公式**

#### **3.1 输入**

解码器的输入是：

* 编码器的**上下文向量** $$c$$。
* 目标序列的前一个 token $$y_{t'-1}$$。

#### **3.2 递归计算**

解码器使用自己的隐藏状态 $$s_t'$$ 进行更新：

$$
s_{t'} = g(s_{t'-1}, y_{t'-1}, c)
$$

其中：

* $$s_{t'}$$ 是解码器的隐藏状态。
* $$g$$ 是解码器的转换函数（通常与编码器的 $$f$$ 结构类似）。

**RNN 版本**：

$$
s_{t'} = \tanh(W_s s_{t'-1} + W_y y_{t'-1} + W_c c + b)
$$

**LSTM 版本**：

$$
f_{t'} = \sigma(W_f [s_{t'-1}, y_{t'-1}, c] + b_f)
$$

$$
i_{t'} = \sigma(W_i [s_{t'-1}, y_{t'-1}, c] + b_i)
$$

$$
o_{t'} = \sigma(W_o [s_{t'-1}, y_{t'-1}, c] + b_o)
$$

$$
\tilde{c}_{t'} = \tanh(W_c [s_{t'-1}, y_{t'-1}, c] + b_c)
$$

$$
c_{t'} = f_{t'} \odot c_{t'-1} + i_{t'} \odot \tilde{c}_{t'}
$$

$$
s_{t'} = o_{t'} \odot \tanh(c_{t'})
$$

#### **3.3 输出生成**

解码器的隐藏状态 $$s_t'$$ 用于计算当前 token $$y_t'$$ 的概率：

$$
P(y_{t'} | y_{t'-1}, \dots , y_1, c) = \text{softmax}(W_o s_{t'} + b_o)
$$

**softmax 计算每个可能单词的概率分布**，并选择最可能的单词作为当前输出。

***

### **4. Encoder 和 Decoder 之间的核心数学区别**

| 方面         | 编码器（Encoder）                      | 解码器（Decoder）                            |
| ---------- | --------------------------------- | --------------------------------------- |
| **输入**     | 输入序列 $$x = (x_1, x_2, ..., x_T)$$ | 编码器的上下文向量 $$c$$ + 目标 token $$y_{t'-1}$$ |
| **隐藏状态计算** | $$h_t = f(h_{t-1}, x_t)$$         | $$s_{t'} = g(s_{t'-1}, y_{t'-1}, c)$$   |
| **输出**     | 最终隐藏状态 $$h_T$$（即上下文向量 $$c$$）      | 生成的 token $$y_{t'}$$                    |
| **目标**     | 压缩整个输入序列的信息                       | 逐步生成目标序列                                |

**数学核心区别：**

1. **输入不同**：
   * 编码器的输入是整个**源语言句子** $$x_1, x_2, ..., x_T$$。
   * 解码器的输入是\*\*编码器输出的上下文向量 $$c$$ + 目标序列的前一个 token $$y_{t'-1}$$。
2. **隐藏状态计算方式不同**：
   * 编码器：$$h_t = f(h_{t-1}, x_t)$$，**仅依赖过去信息**（单向）或**前后信息**（双向）。
   * 解码器：$$s_{t'} = g(s_{t'-1}, y_{t'-1}, c)$$，**依赖于编码器输出 + 先前已生成的 token**。
3. **输出不同**：
   * **编码器输出最终隐藏状态** $$h_T$$**（或经过注意力机制的上下文向量** $$c$$**）**，用作整个输入的表示。
   * **解码器使用 softmax 生成目标序列中的 token**，逐步预测下一个单词。

***

### **5. 总结**

1. **编码器** 负责 **处理整个输入序列**，并生成一个固定长度的**上下文向量** $$c$$，用来表示整个输入序列的信息。
2. **解码器** 逐步**接收上下文向量** $$c$$，并**依赖前一个时间步的输出**，一步步生成目标序列。
3. **数学层面上**：
   * **编码器是一个标准的递归网络**，输入的是 **源语言序列**，只依赖于 **过去信息**（或前后信息，若是双向）。
   * **解码器依赖编码器的输出 + 先前已生成的 token**，是一个 **自回归模型**，用 softmax 生成下一个 token。



## h, x, s, y到底是啥

在 **序列到序列（seq2seq）** 任务中，编码器的最终隐藏状态 $$h_T$$ 被称为**上下文变量或上下文向量（context vector）**。它是整个输入序列的信息压缩，并传递给解码器，以生成目标输出。

***

### **例子：机器翻译**

我们以 **英语到法语翻译** 为例：

* **输入句子（英文）**："I love you"
* **目标句子（法语）**："Je t'aime"

#### **1. 词向量表示**

假设：

* `"I"` → $$x_1$$
* `"love"` → $$x_2$$
* `"you"` → $$x_3$$

这些 $$x_t$$ 是输入单词的**词向量**（word embeddings）。

***

### **2. 编码器（Encoder）**

编码器是一个 **循环神经网络（RNN, LSTM, GRU）**，它逐步读取输入单词，并更新**隐藏状态（hidden state）** $$h_t$$：

1.  **输入第一个单词 `"I"`**：

    $$
    h_1 = f(x_1, h_0)
    $$

    其中 $$h_0$$ 是初始隐藏状态（通常设为全零）。
2.  **输入第二个单词 `"love"`**：

    $$
    h_2 = f(x_2, h_1)
    $$
3.  **输入第三个单词 `"you"`**：

    $$
    h_3 = f(x_3, h_2)
    $$

最终，编码器的**最后一个隐藏状态** $$h_T$$ 作为**上下文向量**：

$$
h_T = h_3
$$

它浓缩了整个输入句子的意思，并作为解码器的输入。

***

### **3. 解码器（Decoder）**

解码器是另一个 RNN，它接收 **上下文向量 ( h\_T )** 作为初始状态，并逐步生成目标序列（"Je t'aime"）：

1.  **生成第一个单词** `"Je"`：

    $$
    s_1 = g(y_0, h_T)
    $$

    其中 $$y_0$$ 是起始标记 `<SOS>`。
2.  **生成第二个单词** `"t'"`：

    $$
    s_2 = g(y_1, s_1)
    $$

    其中 $$y_1$$ 是 `"Je"`。
3.  **生成第三个单词** `"aime"`：

    $$
    s_3 = g(y_2, s_2)
    $$

    其中 $$y_2$$ 是 `"t'"`。

最终，解码器输出 `"Je t'aime"`，完成翻译。

***

### **4. 关键概念总结**

| 符号      | 解释                          |
| ------- | --------------------------- |
| $$x_t$$ | **输入单词的词向量**（如 `"I" → x_1`） |
| $$h_t$$ | **隐藏状态**，编码器在每个时间步更新它       |
| $$h_T$$ | **上下文向量**，编码器的最后隐藏状态        |
| $$s_t$$ | **解码器的隐藏状态**                |
| $$y_t$$ | **解码器的输出单词**                |

***

### **5. 图示**

```plaintext
Encoder:
"I"   → x₁ → h₁ →
"love" → x₂ → h₂ →
"you"  → x₃ → h₃  (最终的上下文向量 h_T)

Decoder:
h_T → "Je"  → "t'" → "aime"
```
