# Decoder Deep Dive

## 解码器（Decoder）

### 1. 解码器的输入与目标

**解码器的作用**：

* **输入**：编码器（Encoder）生成的 **上下文向量 c**，以及前一个时间步的输出。
* **输出**：一个新的序列 $$y_1, y_2, \dots, y_U$$（可能与输入长度不同），比如：
  * 机器翻译时，$$y$$ 是目标语言的翻译句子。
  * 文本摘要时，$$y$$ 是摘要文本。
  * 语音识别时，$$y$$ 是转换后的文本。

**公式：**

$$
y = (y_1, y_2, \cdots , y_U)
$$

其中 $$U$$ 可能与输入序列长度 $$T$$ 不同。

***

### 2. 解码器的隐藏状态更新

解码器在每个时间步 $$t$$ 计算一个新的隐藏状态：

$$
s_t' = g(s_{t-1}, y_{t'-1}, c)
$$

**含义**：

* $$s_t'$$：当前时间步 $$t$$ 的隐藏状态。
* $$s_{t-1}$$：前一个时间步的隐藏状态（用来存储之前的信息）。
* $$y_{t'-1}$$：上一个时间步的输出（解码器是**自回归的**，即它使用自己生成的前一个 token 作为输入）。
* $$c$$：来自编码器的**上下文向量**，表示整个输入序列的信息。

**直观理解**：

* 解码器从 **编码器的输出（context vector）+ 之前生成的 token** 中学习信息。
* **当前隐藏状态** $$s_t'$$ **是基于过去的内容逐步更新的**，类似于 RNN 的递归计算过程。

***

### 3. 生成下一个单词

解码器的隐藏状态 $$s_t'$$ 会传递到输出层，计算当前时间步的 token $$y_{t'}$$ 的条件概率：

$$
P(y_{t'} | y_{t'-1}, \cdots , y_1, c) = \text{softmax}(s_{t-1}, y_{t'-1}, c)
$$

**解释**：

* 解码器会预测下一个 token $$y_{t'}$$ 的**概率分布**，这里使用了 **softmax** 归一化，使得所有可能的单词形成一个概率分布。
* 例如，如果在机器翻译任务中：
  * $$P(\text{"Hello"} | \text{"Bonjour"}, c) = 0.8$$
  * $$P(\text{"Hi"} | \text{"Bonjour"}, c) = 0.2$$
  * 说明解码器认为 "Bonjour" 更可能翻译成 "Hello"。

**直观理解**：

1. 解码器每一步预测下一个单词的概率分布。
2. 通过 softmax 选择最有可能的单词。
3. 这个新单词会成为下一步的输入，重复这个过程，直到生成完整句子。

***

### 4. 直观示例

#### **例子 1：机器翻译（英文 → 法文）**

**输入（Encoder）：** `"I love machine learning"`\
**目标（Decoder）：** `"J'aime l'apprentissage automatique"`

1. **编码器** 读取 `"I love machine learning"`，最终输出 **context vector ( c )**。
2. **解码器** 使用 **context vector** $$c$$，并从 `"<start>"` 令牌开始，逐步预测：
   * 预测 `"J'aime"`，作为第一步输出。
   * 预测 `"l'apprentissage"`，作为第二步输出。
   * 预测 `"automatique"`，作为第三步输出。
   * 预测 `"<end>"`，表示解码完成。

#### **例子 2：文本摘要**

**输入（Encoder）：**\
&#xNAN;_"Neural networks are widely used in deep learning. They can model complex patterns and relationships in data."_\
**目标摘要（Decoder）：**\
&#xNAN;_"Neural networks power deep learning."_

* **编码器** 读取整段文本，提取其上下文信息。
* **解码器** 通过 context vector 逐步生成摘要。

***

### 5. 总结

1. **解码器的任务** 是从编码器得到的上下文向量 $$c$$，然后逐步生成新的序列 $$y_1, y_2, \dots, y_U$$。
2. **解码器的隐藏状态** $$s_t'$$依赖于：
   * 之前的隐藏状态 $$s_{t-1}$$
   * 之前的输出 $$y_{t'-1}$$
   * 上下文向量 $$c$$
3. **解码器通过 softmax 计算下一个 token 的概率分布**，然后选择最可能的 token 作为输出。
4. **它是一个自回归（Autoregressive）过程**，每个新生成的单词都会作为下一步的输入，直到生成完整的序列。

**应用场景**：

* 机器翻译（NMT）
* 文本摘要
* 语音识别（ASR）
* 聊天机器人（Chatbot）

