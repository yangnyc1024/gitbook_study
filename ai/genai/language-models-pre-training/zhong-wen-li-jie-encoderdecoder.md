# Encoder Deep Dive

## 编码器（Encoder）

### 1. 编码器的输入是什么？

编码器的输入是一个**序列**，比如一句话："I love machine learning."\
它会被拆分成一个个单词或子单词（token）： $$x_1, x_2, \cdots, x_T$$ 这些 token 会被转换为**向量表示**（embedding），即： $$\text{vectors } x_1, x_2, \dots, x_T$$

***

### 2. 编码器如何处理输入？

编码器的核心是一个**RNN**（或 LSTM、GRU）。\
它按照时间步 ( t ) 依次读取输入，并维护一个**隐藏状态（hidden state）**：

$$h_t = f(h_{t-1}, x_t)$$

其中：

* $$h_t$$：第 $$t$$ 个时间步的隐藏状态（可以看作“记忆”）
* $$x_t$$：第 $$t$$ 个时间步的输入（即当前单词的向量表示）
* $$h_{t-1}$$：上一个时间步的隐藏状态（携带了前面单词的信息）
* $$f$$：一个 RNN 变换函数，比如 LSTM、GRU 等

**直观理解**：

* 每次编码器处理一个输入单词 $$x_t$$ 时，它结合了之前的信息 $$h_{t-1}$$，然后更新自己的记忆状态 $$h_t$$。
  * <mark style="color:purple;">这里的x是一个单词，而h是一整段话</mark>
* 最终，编码器把整个输入序列的信息压缩到**最后一个隐藏状态** $$h_T$$，用来代表整句话的意义。

***

### 3. 上下文向量（Context Vector）

编码器的最终隐藏状态 $$h_T$$ 叫做 **context vector**，它是整个输入序列的信息摘要：

$$c = m(h_1, \cdots, h_T) = h_T$$

在最简单的情况下，$$m$$ 只是直接取最后的隐藏状态 $$h_T$$，用于后续解码（Decoder）。

**直观理解**：

* 你把一整句话输入编码器，它会一步步处理，每个步骤都会更新记忆状态。
* 最终，编码器会输出一个**总结性的信息向量**（上下文向量 $$c$$），这个向量包含了整个句子的主要信息。
* 这个信息向量接下来可以传递给 **解码器（Decoder）**，用于生成翻译、摘要等任务。

***

### 4. 为什么需要双向编码器（Bidirectional Encoder）？

传统的 RNN 是**单向的**，即它只能从前往后读取输入。因此，它的隐藏状态 $$h_t$$ 只能依赖于之前的单词，**无法看到后面的单词**。

**双向（Bidirectional）编码器** 解决了这个问题。\
它的隐藏状态不仅依赖于前一个单词，还会看**后面的单词**：

$$h_t = f(h_{t-1}, x_t, h_{t+1})$$

**直观理解**：

* 例如，在翻译句子 _"He is ..."_ 时，单向编码器只能看到 _"He is"_，但不知道后面要说什么。
* **双向编码器** 能同时看到 _"He is"_ **和** 后面的词，从而更好地理解上下文。

***

### 5. 总结

1. **编码器（Encoder）** 处理输入序列，并将信息存储到隐藏状态（hidden state）。
2. **最终隐藏状态（context vector）** 可以看作是整个输入序列的“总结”。
3. **单向编码器** 只利用过去的信息，而**双向编码器** 还能利用未来的信息，从而更全面地理解句子。





注意，所以encoder的模型，输出不是一个词，而是一个序列

